{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oaU9_sOGzRD1"
      },
      "source": [
        "# Neural Networks\n",
        "In this notebook we will learn how to train a simple Multilayer Perceptron for image classification using PyTorch. You can find additional information [here](https://pytorch.org/tutorials/beginner/basics/intro.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_o30wUG8zRD7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\ritam\\appdata\\roaming\\python\\python311\\site-packages (2.0.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\ritam\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\ritam\\appdata\\roaming\\python\\python311\\site-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\ritam\\appdata\\roaming\\python\\python311\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\ritam\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\ritam\\appdata\\roaming\\python\\python311\\site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ritam\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ritam\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dvfTDUXuzRD9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BDBhCwhszREA"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w5IQVCmOzREB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5824947.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 6150832.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# torchvision has some datasets already included, so we will load MNIST through torchvision\n",
        "# first we need to define the transformations\n",
        "\n",
        "data_aug = transforms.Compose([transforms.ToTensor()]) # the ToTensor transform scales the image into [0., 1.0] range\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=data_aug,\n",
        ")\n",
        "validation_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=data_aug,\n",
        ")\n",
        "indices = list(range(len(validation_data)))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "test_size = 0.2 * len(indices)\n",
        "split = int(np.floor(test_size))\n",
        "val_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ByDvrH3z1uOT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "5\n",
            "torch.Size([64, 1, 28, 28])\n",
            "tensor([6, 4, 9, 7, 6, 3, 2, 7, 6, 1, 5, 3, 9, 5, 0, 5, 1, 9, 5, 2, 1, 7, 2, 7,\n",
            "        1, 0, 5, 4, 9, 6, 8, 7, 7, 8, 0, 7, 6, 1, 2, 2, 4, 8, 7, 2, 8, 4, 0, 0,\n",
            "        6, 1, 0, 2, 4, 2, 0, 6, 8, 7, 7, 7, 3, 2, 6, 1])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# now we need to define a Dataloader, which allows us to automatically batch our inputs, do sampling and multiprocess data loading\n",
        "batch_size = 64\n",
        "num_workers = 2 # how many processes are used to load the data\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=val_sampler, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
        "test_dataloader = DataLoader(validation_data, sampler=test_sampler, batch_size=1, shuffle=False, num_workers=num_workers, drop_last=False)\n",
        "\n",
        "# let's visualize the data\n",
        "# alternative 1: using the Dataset\n",
        "sample = training_data[0] \n",
        "img = sample[0]\n",
        "label = sample[1]\n",
        "print(img.shape) # note that here we only get one image and its label\n",
        "print(label)\n",
        "\n",
        "# alternative 2: iterate over the Dataloader\n",
        "for batch in train_dataloader:\n",
        "  imgs = batch[0]\n",
        "  labels = batch[1]\n",
        "  print(imgs.shape)\n",
        "  print(labels)\n",
        "\n",
        "  plt.imshow(imgs[0][0,:,:], cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0aqWR_0VzRED"
      },
      "source": [
        "## Defining the model\n",
        "\n",
        "Create an MLP with the following structure: \n",
        "\n",
        "1. Dense/linear layer that takes the images as a flattened input vector and generates an output of 512 of dimension.\n",
        "2. ReLU activation layer\n",
        "3. Dense/linear layer with 512 input and output\n",
        "3. ReLU activation layer\n",
        "4. Dense/linear layer with 10 output channels (10 classes of MNIST)\n",
        "\n",
        "You can use PyTorch's layers: https://pytorch.org/docs/stable/nn.html (Conv2d, ReLU, Linear, MaxPool2d, Dropout, Flatten)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DfK3c9RSzRED"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device) # put model in device (GPU or CPU)\n",
        "print(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpret the implemented architecture and try to answer the following questions:\n",
        "\n",
        "a) What is the shape (width, and # of channels) of the output tensor after the first layer?\n",
        "\n",
        "b) And after the first 3 layers (dense+dense+dense)?\n",
        "\n",
        "c) How many parameters (weights) does the model have? Contrary to Keras, PyTorch does not have an official method for counting the number of parameters of a model, but you can use [torchsummary](https://pypi.org/project/torch-summary/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Equation: w*x + b, thus we have w * #_dimensions + #_bias\")\n",
        "print(f\"1st Dense Layer: {784 * 512 + 512}\")\n",
        "print(f\"2nd Dense Layer: {512 * 512 + 512}\")\n",
        "print(f\"3rd Dense Layer: {512 * 10 + 10}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Flatten: 1-1                           [-1, 784]                 --\n",
            "├─Sequential: 1-2                        [-1, 10]                  --\n",
            "|    └─Linear: 2-1                       [-1, 512]                 401,920\n",
            "|    └─ReLU: 2-2                         [-1, 512]                 --\n",
            "|    └─Linear: 2-3                       [-1, 512]                 262,656\n",
            "|    └─ReLU: 2-4                         [-1, 512]                 --\n",
            "|    └─Linear: 2-5                       [-1, 10]                  5,130\n",
            "==========================================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 1.34\n",
            "==========================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.01\n",
            "Params size (MB): 2.55\n",
            "Estimated Total Size (MB): 2.57\n",
            "==========================================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "├─Flatten: 1-1                           [-1, 784]                 --\n",
              "├─Sequential: 1-2                        [-1, 10]                  --\n",
              "|    └─Linear: 2-1                       [-1, 512]                 401,920\n",
              "|    └─ReLU: 2-2                         [-1, 512]                 --\n",
              "|    └─Linear: 2-3                       [-1, 512]                 262,656\n",
              "|    └─ReLU: 2-4                         [-1, 512]                 --\n",
              "|    └─Linear: 2-5                       [-1, 10]                  5,130\n",
              "==========================================================================================\n",
              "Total params: 669,706\n",
              "Trainable params: 669,706\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 1.34\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.01\n",
              "Params size (MB): 2.55\n",
              "Estimated Total Size (MB): 2.57\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!pip install torch-summary\n",
        "from torchsummary import summary\n",
        "summary(model, (1, 28, 28))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g1kRGiw_zREE"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNa9_1jhzREE"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss() # already includes the Softmax activation\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxSdayviCWk5"
      },
      "outputs": [],
      "source": [
        "def epoch_iter(dataloader, model, loss_fn, optimizer=None, is_train=True):\n",
        "    if is_train:\n",
        "      assert optimizer is not None, \"When training, please provide an optimizer.\"\n",
        "      \n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    if is_train:\n",
        "      model.train() # put model in train mode\n",
        "    else:\n",
        "      model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.set_grad_enabled(is_train):\n",
        "      for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # Compute prediction error\n",
        "          pred = model(X)\n",
        "          loss = loss_fn(pred, y)\n",
        "\n",
        "          if is_train:\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "          probs = F.softmax(pred, dim=1)\n",
        "          final_pred = torch.argmax(probs, dim=1)\n",
        "\n",
        "          # Save training metrics\n",
        "          total_loss += loss.item() # IMPORTANT: call .item() to obtain the value of the loss WITHOUT the computational graph attached\n",
        "\n",
        "          preds.extend(final_pred.cpu().numpy())\n",
        "          labels.extend(y.cpu().numpy())\n",
        "\n",
        "    return total_loss / num_batches, accuracy_score(labels, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmsUVGS6C0O1"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "train_history = {'loss': [], 'accuracy': []}\n",
        "val_history = {'loss': [], 'accuracy': []}\n",
        "best_val_loss = np.inf\n",
        "print(\"Start training...\")\n",
        "for t in range(num_epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    train_loss, train_acc = epoch_iter(train_dataloader, model, loss_fn, optimizer)\n",
        "    print(f\"Train loss: {train_loss:.3f} \\t Train acc: {train_acc:.3f}\")\n",
        "    val_loss, val_acc = epoch_iter(validation_dataloader, model, loss_fn, is_train=False)\n",
        "    print(f\"Val loss: {val_loss:.3f} \\t Val acc: {val_acc:.3f}\")\n",
        "\n",
        "    # save model when val loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
        "      torch.save(save_dict, 'best_model.pth')\n",
        "\n",
        "    # save latest model\n",
        "    save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
        "    torch.save(save_dict, 'latest_model.pth')\n",
        "\n",
        "    # save training history for plotting purposes\n",
        "    train_history[\"loss\"].append(train_loss)\n",
        "    train_history[\"accuracy\"].append(train_acc)\n",
        "\n",
        "    val_history[\"loss\"].append(val_loss)\n",
        "    val_history[\"accuracy\"].append(val_acc)\n",
        "    \n",
        "print(\"Finished\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WrQMAKFHzREG"
      },
      "source": [
        "## Analyse training evolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr48TEVlzREH"
      },
      "outputs": [],
      "source": [
        "def plotTrainingHistory(train_history, val_history):\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(train_history['loss'], label='train')\n",
        "    plt.plot(val_history['loss'], label='val')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(train_history['accuracy'], label='train')\n",
        "    plt.plot(val_history['accuracy'], label='val')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GfeNPc4zREI"
      },
      "outputs": [],
      "source": [
        "plotTrainingHistory(train_history, val_history)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HPZLw5cfzREI"
      },
      "source": [
        "## Test the model\n",
        "\n",
        "Evaluate the model in the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtmFHipizREK"
      },
      "outputs": [],
      "source": [
        "# load the best model (i.e. model with the lowest val loss...might not be the last model)\n",
        "# we could also load the optimizer and resume training if needed\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "test_loss, test_acc = epoch_iter(test_dataloader, model, loss_fn, is_train=False)\n",
        "print(f\"\\nTest Loss: {test_loss:.3f} \\nTest Accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1yyEC9pzREJ"
      },
      "outputs": [],
      "source": [
        "def showErrors(model, dataloader, num_examples=20):    \n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    for ind, (X, y) in enumerate(dataloader):\n",
        "      if ind >= 20: break\n",
        "      X, y = X.to(device), y.to(device)    \n",
        "      pred = model(X)\n",
        "      probs = F.softmax(pred, dim=1)\n",
        "      final_pred = torch.argmax(probs, dim=1)\n",
        "\n",
        "      plt.subplot(10, 10, ind + 1)\n",
        "      plt.axis(\"off\")\n",
        "      plt.text(0, -1, y[0].item(), fontsize=14, color='green') # correct\n",
        "      plt.text(8, -1, final_pred[0].item(), fontsize=14, color='red')  # predicted\n",
        "      plt.imshow(X[0][0,:,:].cpu(), cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh38evzTzREL"
      },
      "outputs": [],
      "source": [
        "showErrors(model, test_dataloader)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-C6glKseuO"
      },
      "source": [
        "## Additional Challenges\n",
        "\n",
        "a) As the test accuracy should show, the MNIST dataset is not very challenging, change the code to use Fashion-MNIST and compare the results.\n",
        "\n",
        "b) Do the same for the CIFAR10 (or CIFAR100) dataset. Note that, in this case, each image is a 32x32 color image; convert it to grayscale or concatenate the RGB channels in one single vector (e.g. using the reshape method).\n",
        "\n",
        "c) The test accuracy for CIFAR is significantly worse. Try improving the results by using: 1) a deeper architecture, and 2) a different optmizer.\n",
        "\n",
        "You can load the datasets from [here](https://pytorch.org/vision/stable/datasets.html).\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "nn_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
